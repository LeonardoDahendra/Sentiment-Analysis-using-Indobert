{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "classifier = None\n",
    "FILEPATH = 'indo.csv'\n",
    "import re\n",
    "\n",
    "pattern = r\"[^A-Za-z ]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(word_list):\n",
    "    punc = string.punctuation\n",
    "    removed_symbols = []\n",
    "    for word in word_list:\n",
    "        removed_symbols.append(''.join([let for let in word if let not in punc]))\n",
    "    return removed_symbols\n",
    "\n",
    "def preprocess_data(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(pattern, '', text)\n",
    "    word_list = word_tokenize(text)\n",
    "    in_stopwords = stopwords.words('indonesian')\n",
    "    more_stopwords = ['lu', 'gua', 'i', 'gue', 'lo', 'gw']\n",
    "    word_list = [word for word in word_list if word not in in_stopwords]\n",
    "    word_list = [word for word in word_list if word not in more_stopwords]\n",
    "    word_list = [word for word in word_list if word.isalpha()]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = [lemmatizer.lemmatize(word) for word in word_list]\n",
    "    return word_list\n",
    "\n",
    "def extract_features(dataset):\n",
    "    all_words = []\n",
    "    for text in dataset:\n",
    "        word_list = preprocess_data(text)\n",
    "        all_words.extend(word_list)\n",
    "    fd = FreqDist(all_words)\n",
    "    common_words = [word for word, _ in fd.most_common(1000)]\n",
    "    common_words = list(set(common_words))\n",
    "    return common_words\n",
    "\n",
    "def extract_dataset():\n",
    "    dataset = get_dataset()\n",
    "    word_dictionary = extract_features(dataset['Text'])\n",
    "    document = []\n",
    "    for _, data in dataset.iterrows():\n",
    "        features = {}\n",
    "        word_list = preprocess_data(data['Text'])\n",
    "        for word in word_dictionary:\n",
    "            key = word\n",
    "            value = word in word_list\n",
    "            features[key] = value\n",
    "        \n",
    "        document.append((features, data['Sentiment']))\n",
    "    return document\n",
    "\n",
    "def train_data(dataset):\n",
    "    random.shuffle(dataset)\n",
    "    training_len = int(len(dataset) * 0.7)\n",
    "    training_data = dataset[:training_len]\n",
    "    testing_data = dataset[training_len:]\n",
    "\n",
    "    global classifier\n",
    "    classifier = NaiveBayesClassifier.train(training_data)\n",
    "    print(f\"Accuracy: {accuracy(classifier, testing_data) * 100}%\")\n",
    "\n",
    "    file = open('model.pickle', 'wb')\n",
    "    pickle.dump(classifier, file)\n",
    "    file.close()\n",
    "\n",
    "def predict_comment(comment):\n",
    "    dataset = get_dataset()\n",
    "    word_dictionary = extract_features(dataset['Text'])\n",
    "    word_list = preprocess_data(comment)\n",
    "    features = {}\n",
    "    for word in word_dictionary:\n",
    "        features[word] = word in word_list\n",
    "    print(f\"Prediction: {classifier.classify(features)}\")\n",
    "\n",
    "def get_dataset():\n",
    "    dataset = pd.read_csv(FILEPATH)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.1437908496732%\n"
     ]
    }
   ],
   "source": [
    "dataset = extract_dataset()\n",
    "train_data(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
